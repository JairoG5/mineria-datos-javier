---
title: "Práctica I"
description: |
  Análisis de componentes principales
author:
  - name: Jairo Garcia (DNI Z0020363J)
    affiliation: Universidad Complutense de Madrid
    affiliation_url: 
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        colorlinks: true
        code_folding: false
        toc: true            
        toc_depth: 3     
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = TRUE}
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 9, fig.asp = 1, out.width = "100%",
                      message = FALSE, warning = FALSE,
                      echo = TRUE, res = 400)
```

# Instrucciones (leer antes de empezar)

-   Modifica dentro del documento `.Rmd` tus datos personales (nombre y
    DNI) ubicados en la cabecera del archivo.

-   Asegúrate antes de seguir editando el documento que el archivo
    `.Rmd` compila (Knit) correctamente y se genera el `html`
    correspondiente.

-   Los chunks creados están o vacíos o incompletos, de ahí que tengan
    la opción `eval = FALSE`. Una vez que edites lo que consideres debes
    de cambiar a `eval = TRUE` para que los chunk se ejecuten

## Paquetes necesarios

Necesitaremos los siguientes paquetes:

```{r paquetes}
# Borramos variables del environment
rm(list = ls())
library(readxl)
library(skimr)
library(corrr)
library(corrplot)
library(tidyverse)
library(tidymodels)
library(factoextra)
library(FactoMineR)
library(ggforce)
```

# Carga de datos

El archivo de datos a usar será `distritos.xlsx`

```{r}
distritos <- read_xlsx(path = "./distritos.xlsx")
```

El fichero contiene **información socioeconómica de los distritos de
Madrid**

```{r}
glimpse(distritos)
```

Las variables recopilan la siguiente información:

-   `Superficie`: superficie del distrito (hectáreas)
-   `Densidad`: densidad de población
-   `Pob_0_14`: proporción de población menor de 14 años
-   `Pob_15_29`: proporción de población de 15 a 29
-   `Pob_30_44`: proporción de población de 30 a 44
-   `Pob_45_64`: proporción de población de 45 a 64
-   `Pob_65+`: proporción de población de 65 o mas
-   `N_Española`: proporción de población española
-   `Extranjeros`: proporción de población extranjera
-   `N_ hogares`: número de hogares en miles
-   `Renta`: renta media en miles
-   `T_paro`: porcentaje de población parada
-   `T_paro_H`: porcentaje de hombres parados
-   `T_ paro_M`: porcentaje de mujeres paradas
-   `Paro_LD`: proporción de población parada de larga duración
-   `Analfabetos`: proporción de población que no sabe leer ni escribir
-   `Primaria_ inc`: proporción de población solo con estudios primarios
-   `ESO`: proporción de población solo ESO
-   `fp_bach`: proporción de población solo con FP o Bachillerato
-   `T_medios`: proporción de población Titulada media
-   `T_superiores`: proporción de población con estudios superiores
-   `S_M2_vivienda`: superficie media de la vivienda
-   `Valor_V`: valor catastral medio de la vivienda
-   `Partido`: partido más votado en las municipales 2019

# Ejercicio 1:

> Calcula los estadísticos básicos de todas las variables con la función
> `skim()` del paquete `{skimr}` y detalle debajo del chunk lo que
> consideres.

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
distritos %>% skim()
```

>De todas las variables 23 son numericas, son las que usaremos mas adelante
# Ejercicio 2

## Ejercicio 2.1

> Calcula la matriz de covarianzas. Recuerda que la matriz de
> covarianzas y de correlaciones solo puede ser calculada para variables
> numéricas.

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
cov_mat <-
  cov(distritos %>% select(where(is.numeric)))
cov_mat
```

## Ejercicio 2.2

> Calcula la matriz de correlaciones, de forma numérica y gráfica,
> haciendo uso de `{corrplot}`. Responde además a las preguntas: ¿cuáles
> son las variables más correlacionadas (linealmente)? ¿Cómo es el
> sentido de esa correlación?

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
cor_mat <-
  cor(distritos %>% select(where(is.numeric)))
cor_mat

corrplot(cor_mat, type = "upper",
          tl.col = "black",  method = "ellipse")
```

>Las variables con mayor correlacion son T_paro y T_paro_H, T_paro y T_paro_M, en ambos casos es una relacion positiva.

# Ejercicio 3

> Haciendo uso de `{ggplot2}`, representa los gráficos de dispersión de
> las variables T_paro (eje y) con relación a Analfabetos (eje x), y
> T_paro en relación a T_superiores. Comentar el sentido de las nubes de
> puntos, junto con las correlaciones obtenidas anteriormente. Puedes
> crear otros gráficos que consideres útiles.

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
ggplot(distritos, aes(x = Analfabetos, y = T_paro)) +
  geom_point(size = 7, alpha = 0.6) +
  labs(x = "Analfabetos", y = "T_paro",
       color = "rel",
       title = "Gráficos de dispersión") +
  theme_minimal()
```

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
ggplot(distritos, aes(x = T_superiores, y = T_paro)) +
  geom_point(size = 7, alpha = 0.6) +
  labs(x = "T_paro", y = "T_superiores.",
       title = "Relacion Estudios superiores con T_paro") +
  theme_minimal()
```

>considerando el ejercicio anterior donde pudimos ver la correlacion existente y el sentido de las nubes de punto podemos concluir que: la correlacion entre las variables  “T_paro” y “Analfabetos” (0.95) es muy proxima a ser lineal.
por otro lado la relacion entre “T_paro” y “T_superiores” es de -0.94, hay una relacion no lineal, pero podemos entender que alguien con estudios superiores dificilmente estara en el paro.


# Ejercicio 4

## Ejercicio 4.1

> Haciendo uso de los paquetes `{FactoMineR}` y `{factoextra}`, realiza
> un análisis de componentes principales y guárdalo en el objeto
> `pca_fit`

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
pca_fit <-
   PCA(distritos %>% select(where(is.numeric)), scale.unit = TRUE , ncp = 23, graph = FALSE)
  


```

> Obtén los autovalores asociados y detalla los resultados. ¿Cuánto
> explica la primera componente? ¿Cuánto explican las primeras 10
> componentes? 

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
#descomposicion en valores singulares, singular value decomposition (SVD) svd$V
#Autovalores: para mostrar los autovalores(varianza explicada por cada componente) basta con llamar
#pca_fit$eig(ya nos los da ordenados y con la varianza acumulada)
#para obtener los autovectores get_eig(pca_fit)
pca_fit$eig
```

>Componente 1: 52.11769
las primeras 10 componentes explican el 99.00305%

> Obtén los autovectores por columnas y comenta lo que consideres

```{r eval = TRUE}
# Completa el código
#la suma de todas sus cordenadas al cuadrado me da 1
pca_fit$svd$V
```

> Explícita además la expresión de la primera componente en función de
> las variables originales. 3:36:35

$$\Phi_1 = -0.449 * Superficie - 0.05 * Densidad + 0.05 * Prob_0_14 + 0.167 * Prob_15_29 + 0.009 * Prob_30_44 + 0.155 * Prob_45_64 - 0.170 * Prob_65+ - 0.211 * N_Española + 0.204 * Extranjeros + 0.040 * N_Hogares - 0.273 * Renta + 0.286 * T_paro + 0.285 * T_paro_H + 0.282 * T-paro_M + 0.149 * Paro_LD + 0.275 * Analfabetos + 0.276 * Primaria_inc + 0.282 * ESO + 0.051 * fp_bach - 0.248 * T_medios - 0.274 * T_superiores - 0.199 * S_M2_vivienda - 0.265 * Valor_V$$

> Obtén los scores (las nuevas coordenadas de los datos, proyectados en
> las nuevas direcciones) y comenta lo que consideres 


```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
pca_scores <- as_tibble(pca_fit$ind$coord)
names(pca_scores) <- c("PC_1", "PC_2", "PC_3", "PC_4", "PC_5","PC_6","PC_7","PC_8","PC_9","PC_10","PC_11","PC_12","PC_13","PC_14","PC_15","PC_16","PC_17","PC_18","PC_19","PC_20")
pca_scores # Nuevas coordenadas

```

## Ejercicio 4.2

> Detalla todo lo que consideres sobre las constribuciones de cada
> variable a cada componente.

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
pca_fit$ind$contrib 
```

>La suma de todo da uno, esto es lo que contribuye cada individuo a la componente

> Visualiza la varianza explicada por cada componente haciendo uso de
> `fviz_eig()`

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
fviz_eig(pca_fit,
         barfill = "#5441F5",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

> Construye un gráfico para visualizar la varianza explicada acumulada
> (con una línea horizontal que nos indica el umbral del 95%)

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE

cumvar <- as_tibble(pca_fit$eig)
names(cumvar) <- c("autovalor", "var", "cumvar")

ggplot(cumvar, aes(x = 1:20, y = cumvar)) +
  geom_col(fill = "#239B56") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

> Usando `fviz_pca_var()` visualiza de forma bidimensional como se
> relacionan las variables originales con las dos componentes que mayor
> cantidad de varianza capturan. Detalla los resultados del gráfico todo
> lo que consideres

```{r eval = TRUE}
# Completa el código y cambia a eval = TRUE
col <- c("#00AFBB", "#E7B800", "#FC4E07")
fviz_pca_var(pca_fit, col.var = "cos2",
             gradient.cols = col,
             repel = TRUE) +
  theme_minimal() + 
  labs(title = "Coordenadas de las variables",
       color = "Prop. var. explicada")
```

>Una gran cantidad de variables esta proxima al eje X, que indica que muchas de ellas son representadas con el componente 1.

> Haz `fviz_cos2()` y detalla todo lo que consideres del gráfico

```{r eval = TRUE}
# Completa el código
fviz_cos2(pca_fit, choice = "var",
          axes = 1:2)
```

>La variable peor explicada es N_hogares.


> Con `fviz_pca_biplot()` visualiza en las dos dimensiones que más
> varianza capturan los clústers de observaciones con las elipses
> definidas por las matrices de covarianza de cada uno de los grupos
> (añadiendo el partido más votado en cada distrito en color). Teniendo
> en cuenta el anterior biplot, comentar las características
> socioeconómicas de algunos grupos de distritos.

```{r eval = TRUE}
# Completa el código
fviz_pca_biplot(pca_fit,
                col.ind = distritos$Partido,
                palette = "jco",
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE,
                legend.title = "Partido más votado")



```
>Las personas que votan por el partido Mas Madrid, tienen mas diversidad en muchas areas, edad, niveles de estudios, extranjeros etc, contrario a PP que es descrita con menos variables apuntando a su area, la cual indica que es muy de nicho, con personas mayores de 65 años, con estudios superioes, grandes casas etc.

> ¿Qué valor tiene el distrito de Salamanca en la Componente 1? ¿Y
> Villaverde? ¿Qué distrito tiene un valor más alto de la Componente 4?

```{r}
pca_fit$ind$coor[,1]
```

```{r}
pca_fit$ind$coor[,4]
```
>Salamanca: -4.32
 Villaverde: 5.48




# Ejercicio 5

> Haz uso de tidymodels para calcular las componentes y las 5
> componentes que más varianza capturan en una matriz de gráficas (la
> diagonal la propia densidad de las componentes, fuera de la diagonal
> los datos proyectados en la componente (i,j)). Codifica el color como
> el partido más votado. Al margen de la varianza explicada, ¿qué par de
> componentes podrían servirnos mejor para «clasificar» nuestros barrios
> según el partido más votado?

```{r eval = TRUE}
receta <- 
  recipe(Partido  ~ ., data = distritos) %>%
  # Imputamos por la media las numéricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # Estandarizamos
  step_normalize(all_numeric_predictors()) %>%
  # Filtro cero varianza
  step_zv(all_numeric_predictors())
receta



receta <-
  receta %>%
  step_pca(all_numeric_predictors(), num_comp = 5,
           prefix = "PC") 


data_pc <- bake(receta %>% prep(), new_data = NULL)
data_pc


ggplot(data_pc,
       aes(x = .panel_x, y = .panel_y,
           color = Partido, fill = Partido)) +
  geom_point(alpha = 0.4, size = 0.9) +
  ggforce::geom_autodensity(alpha = 0.3) +
  ggforce::facet_matrix(vars(-c(Distrito, Partido)), layer.diag = 2) + 
  scale_color_brewer(palette = "Dark2") + 
  scale_fill_brewer(palette = "Dark2") +
  theme_minimal() +
  labs(title = "PCA con tidymodels")



```

>Los componentes PC1 y PC4 son los componentes que podrían servirnos mejor para «clasificar» nuestros barrios según el partido más votado.

# Ejercicio 6 (opcional)

> Comenta todo lo que consideres tras un análisis numérico y visual, y
> que no haya sido preguntado
